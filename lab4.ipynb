{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHtaPysVSNZN"
   },
   "source": [
    "# Step 1 - Install the required dependencies and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "APS6D3eiSAR_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zenoml\n",
      "  Downloading zenoml-0.6.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fastapi<0.101.0,>=0.95.1 (from zenoml)\n",
      "  Downloading fastapi-0.100.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting inquirer<4.0.0,>=3.1.2 (from zenoml)\n",
      "  Downloading inquirer-3.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nest-asyncio<2.0.0,>=1.5.6 (from zenoml)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting opentsne<1.1.0,>=0.7.1 (from zenoml)\n",
      "  Downloading openTSNE-1.0.2-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting pandas<3.0.0,>=2.0.0 (from zenoml)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pathos<0.4.0,>=0.3.0 (from zenoml)\n",
      "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic<2.0.0 (from zenoml)\n",
      "  Downloading pydantic-1.10.21-cp310-cp310-win_amd64.whl.metadata (155 kB)\n",
      "Collecting requests<3.0.0,>=2.28.1 (from zenoml)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from zenoml) (75.8.0)\n",
      "Collecting tomli<3.0.0,>=2.0.1 (from zenoml)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tqdm<5.0.0,>=4.64.0 (from zenoml)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting uvicorn<0.24,>=0.22 (from zenoml)\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websockets<12.0,>=11.0 (from zenoml)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting zeno-sliceline<0.0.2,>=0.0.1 (from zenoml)\n",
      "  Downloading zeno_sliceline-0.0.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.101.0,>=0.95.1->zenoml)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from fastapi<0.101.0,>=0.95.1->zenoml) (4.12.2)\n",
      "Collecting blessed>=1.19.0 (from inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting editor>=1.6.0 (from inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading editor-1.6.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting readchar>=4.2.0 (from inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting numpy>=1.16.6 (from opentsne<1.1.0,>=0.7.1->zenoml)\n",
      "  Downloading numpy-2.2.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn>=0.20 (from opentsne<1.1.0,>=0.7.1->zenoml)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from opentsne<1.1.0,>=0.7.1->zenoml)\n",
      "  Downloading scipy-1.15.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3.0.0,>=2.0.0->zenoml) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0.0,>=2.0.0->zenoml)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=2.0.0->zenoml)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting ppft>=1.7.6.9 (from pathos<0.4.0,>=0.3.0->zenoml)\n",
      "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.9 (from pathos<0.4.0,>=0.3.0->zenoml)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.5 (from pathos<0.4.0,>=0.3.0->zenoml)\n",
      "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.17 (from pathos<0.4.0,>=0.3.0->zenoml)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.28.1->zenoml)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.28.1->zenoml)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.28.1->zenoml)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.28.1->zenoml)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.64.0->zenoml) (0.4.5)\n",
      "Collecting click>=7.0 (from uvicorn<0.24,>=0.22->zenoml)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn<0.24,>=0.22->zenoml)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting numpy>=1.16.6 (from opentsne<1.1.0,>=0.7.1->zenoml)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from blessed>=1.19.0->inquirer<4.0.0,>=3.1.2->zenoml) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from blessed>=1.19.0->inquirer<4.0.0,>=3.1.2->zenoml) (1.16.0)\n",
      "Collecting jinxed>=1.1.0 (from blessed>=1.19.0->inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading jinxed-1.3.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting runs (from editor>=1.6.0->inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading runs-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xmod (from editor>=1.6.0->inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading xmod-1.8.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=0.20->opentsne<1.1.0,>=0.7.1->zenoml)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.20->opentsne<1.1.0,>=0.7.1->zenoml)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi<0.101.0,>=0.95.1->zenoml)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.101.0,>=0.95.1->zenoml) (1.2.0)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.101.0,>=0.95.1->zenoml)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ansicon (from jinxed>=1.1.0->blessed>=1.19.0->inquirer<4.0.0,>=3.1.2->zenoml)\n",
      "  Downloading ansicon-1.89.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading zenoml-0.6.4-py3-none-any.whl (704 kB)\n",
      "   --------------------------------------- 704.1/704.1 kB 14.3 MB/s eta 0:00:00\n",
      "Downloading fastapi-0.100.1-py3-none-any.whl (65 kB)\n",
      "Downloading inquirer-3.4.0-py3-none-any.whl (18 kB)\n",
      "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading openTSNE-1.0.2-cp310-cp310-win_amd64.whl (470 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 11.6/11.6 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
      "Downloading pydantic-1.10.21-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 2.3/2.3 MB 14.5 MB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "Downloading websockets-11.0.3-cp310-cp310-win_amd64.whl (124 kB)\n",
      "Downloading zeno_sliceline-0.0.1-py3-none-any.whl (18 kB)\n",
      "Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Downloading editor-1.6.6-py3-none-any.whl (4.0 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 16.6 MB/s eta 0:00:00\n",
      "Downloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 11.1/11.1 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "   ---------------------------------------- 43.9/43.9 MB 13.0 MB/s eta 0:00:00\n",
      "Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading jinxed-1.3.0-py2.py3-none-any.whl (33 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading runs-1.2.2-py3-none-any.whl (7.0 kB)\n",
      "Downloading xmod-1.8.1-py3-none-any.whl (4.6 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading ansicon-1.89.0-py2.py3-none-any.whl (63 kB)\n",
      "Installing collected packages: pytz, ansicon, xmod, websockets, urllib3, tzdata, tqdm, tomli, threadpoolctl, sniffio, readchar, pydantic, ppft, pox, numpy, nest-asyncio, joblib, jinxed, idna, h11, dill, click, charset-normalizer, certifi, uvicorn, scipy, runs, requests, pandas, multiprocess, blessed, anyio, starlette, scikit-learn, pathos, editor, zeno-sliceline, opentsne, inquirer, fastapi, zenoml\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.5\n",
      "    Uninstalling nest-asyncio-1.5.5:\n",
      "      Successfully uninstalled nest-asyncio-1.5.5\n",
      "Successfully installed ansicon-1.89.0 anyio-4.8.0 blessed-1.20.0 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 dill-0.3.9 editor-1.6.6 fastapi-0.100.1 h11-0.14.0 idna-3.10 inquirer-3.4.0 jinxed-1.3.0 joblib-1.4.2 multiprocess-0.70.17 nest-asyncio-1.6.0 numpy-1.26.4 opentsne-1.0.2 pandas-2.2.3 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9 pydantic-1.10.21 pytz-2025.1 readchar-4.2.1 requests-2.32.3 runs-1.2.2 scikit-learn-1.6.1 scipy-1.15.1 sniffio-1.3.1 starlette-0.27.0 threadpoolctl-3.5.0 tomli-2.2.1 tqdm-4.67.1 tzdata-2025.1 urllib3-2.3.0 uvicorn-0.23.2 websockets-11.0.3 xmod-1.8.1 zeno-sliceline-0.0.1 zenoml-0.6.4\n"
     ]
    }
   ],
   "source": [
    "!pip install zenoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (21.3)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.66.3->datasets) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.12-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.0-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 25.3/25.3 MB 16.0 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, pyyaml, pyarrow, propcache, multidict, fsspec, frozenlist, filelock, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.17\n",
      "    Uninstalling multiprocess-0.70.17:\n",
      "      Successfully uninstalled multiprocess-0.70.17\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 datasets-3.2.0 dill-0.3.8 filelock-3.17.0 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.28.1 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-19.0.0 pyyaml-6.0.2 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.16 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 9.7/9.7 MB 20.8 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.2\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.5)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.6.0-cp310-cp310-win_amd64.whl (204.2 MB)\n",
      "   --------------------------------------- 204.2/204.2 MB 16.7 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 6.2/6.2 MB 11.9 MB/s eta 0:00:00\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRrMiMnLV9xY",
    "outputId": "5193e819-f2cb-4032-99df-ced1ea7b4191",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: â€œI had a blue penis while I was thisâ€ [pla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I may be finally in with the in crowd ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  @user @user what do these '1/2 naked pics' hav...      1\n",
       "1  OH: â€œI had a blue penis while I was thisâ€ [pla...      1\n",
       "2  @user @user That's coming, but I think the vic...      1\n",
       "3  I think I may be finally in with the in crowd ...      2\n",
       "4  @user Wow,first Hugo Chavez and now Fidel Cast...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "df = pd.DataFrame(ds['test']).head(500)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(x):\n",
    "    if x == 0:\n",
    "        return 'negative'\n",
    "    elif x == 1:\n",
    "        return 'neutral'\n",
    "    elif x == 2:\n",
    "        return 'positive'\n",
    "    return x\n",
    "df['label'] = df['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Run model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: This step is going to download two models of ~500MB each. \n",
    "\n",
    "**If you don't want to download the models, you can jump to step 4 and use the provided data in the repo instead.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference with roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:37<00:00, 13.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "results = []\n",
    "texts = df['text'].to_list()\n",
    "\n",
    "## Depending on your machine, this should take around 1 minute\n",
    "for text in tqdm.tqdm(texts):\n",
    "    results.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['roberta'] = [r[0]['label'] for r in results]\n",
    "df['roberta_score'] = [r[0]['score'] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference with gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"LYTinn/finetuning-sentiment-model-tweet-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:37<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "results = []\n",
    "texts = df['text'].to_list()\n",
    "\n",
    "## Depending on your machine, this should take around 1 minute\n",
    "for text in tqdm.tqdm(texts):\n",
    "    results.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gpt2'] = [r[0]['label'] for r in results]\n",
    "df['gpt2_score'] = [r[0]['score'] for r in results]\n",
    "\n",
    "## map labels back\n",
    "def label_map(x):\n",
    "    if x == 'LABEL_0':\n",
    "        return 'negative'\n",
    "    elif x == 'LABEL_1':\n",
    "        return 'neutral'\n",
    "    elif x == 'LABEL_2':\n",
    "        return 'positive'\n",
    "    return x\n",
    "df['gpt2'] = df['gpt2'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Pre-processing data and add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you skip the model inference, uncomment the code below and load the provided data\n",
    "\n",
    "# df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input_length\"] = df[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Start Zeno for interactive slicing\n",
    "\n",
    "In this step, you need to create 5 slices in the Zeno interface and derive meaningful insights.\n",
    "\n",
    "As a starting point, try to create the two slices we provide:\n",
    "\n",
    "1. Tweets with hashtags\n",
    "2. Tweets with strong positive words (e.g., love) -- you can determine the exact words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating slices in Zeno is straightforward: Just click on the '+' button for 'create a new slice', and you can define the slice using existing column attributes, with simple value macthing or even regular expression.\n",
    "\n",
    "![image.png](images/image.png)\n",
    "\n",
    "There are more fun features in Zeno, including interactive metadata & model comparison -- feel free to check the teaser video in [README](https://github.com/zeno-ml/zeno) of the Zeno repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created project.\n",
      "Access your project at  https://hub.zenoml.com/project/70b07022-ff5d-45a2-8d4f-9e36c0651f4e/Tweet%20Sentiment%20Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\aie_4\\lib\\site-packages\\zeno_client\\util.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15'\n",
      " '16' '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n",
      " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
      " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
      " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
      " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99'\n",
      " '100' '101' '102' '103' '104' '105' '106' '107' '108' '109' '110' '111'\n",
      " '112' '113' '114' '115' '116' '117' '118' '119' '120' '121' '122' '123'\n",
      " '124' '125' '126' '127' '128' '129' '130' '131' '132' '133' '134' '135'\n",
      " '136' '137' '138' '139' '140' '141' '142' '143' '144' '145' '146' '147'\n",
      " '148' '149' '150' '151' '152' '153' '154' '155' '156' '157' '158' '159'\n",
      " '160' '161' '162' '163' '164' '165' '166' '167' '168' '169' '170' '171'\n",
      " '172' '173' '174' '175' '176' '177' '178' '179' '180' '181' '182' '183'\n",
      " '184' '185' '186' '187' '188' '189' '190' '191' '192' '193' '194' '195'\n",
      " '196' '197' '198' '199' '200' '201' '202' '203' '204' '205' '206' '207'\n",
      " '208' '209' '210' '211' '212' '213' '214' '215' '216' '217' '218' '219'\n",
      " '220' '221' '222' '223' '224' '225' '226' '227' '228' '229' '230' '231'\n",
      " '232' '233' '234' '235' '236' '237' '238' '239' '240' '241' '242' '243'\n",
      " '244' '245' '246' '247' '248' '249' '250' '251' '252' '253' '254' '255'\n",
      " '256' '257' '258' '259' '260' '261' '262' '263' '264' '265' '266' '267'\n",
      " '268' '269' '270' '271' '272' '273' '274' '275' '276' '277' '278' '279'\n",
      " '280' '281' '282' '283' '284' '285' '286' '287' '288' '289' '290' '291'\n",
      " '292' '293' '294' '295' '296' '297' '298' '299' '300' '301' '302' '303'\n",
      " '304' '305' '306' '307' '308' '309' '310' '311' '312' '313' '314' '315'\n",
      " '316' '317' '318' '319' '320' '321' '322' '323' '324' '325' '326' '327'\n",
      " '328' '329' '330' '331' '332' '333' '334' '335' '336' '337' '338' '339'\n",
      " '340' '341' '342' '343' '344' '345' '346' '347' '348' '349' '350' '351'\n",
      " '352' '353' '354' '355' '356' '357' '358' '359' '360' '361' '362' '363'\n",
      " '364' '365' '366' '367' '368' '369' '370' '371' '372' '373' '374' '375'\n",
      " '376' '377' '378' '379' '380' '381' '382' '383' '384' '385' '386' '387'\n",
      " '388' '389' '390' '391' '392' '393' '394' '395' '396' '397' '398' '399'\n",
      " '400' '401' '402' '403' '404' '405' '406' '407' '408' '409' '410' '411'\n",
      " '412' '413' '414' '415' '416' '417' '418' '419' '420' '421' '422' '423'\n",
      " '424' '425' '426' '427' '428' '429' '430' '431' '432' '433' '434' '435'\n",
      " '436' '437' '438' '439' '440' '441' '442' '443' '444' '445' '446' '447'\n",
      " '448' '449' '450' '451' '452' '453' '454' '455' '456' '457' '458' '459'\n",
      " '460' '461' '462' '463' '464' '465' '466' '467' '468' '469' '470' '471'\n",
      " '472' '473' '474' '475' '476' '477' '478' '479' '480' '481' '482' '483'\n",
      " '484' '485' '486' '487' '488' '489' '490' '491' '492' '493' '494' '495'\n",
      " '496' '497' '498' '499']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, id_column] = df[id_column].astype(str)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.02s/it]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15928\\1905128310.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_system[\"correct\"] = (df_system[model] == df[\"label\"]).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.58it/s]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15928\\1905128310.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_system[\"correct\"] = (df_system[model] == df[\"label\"]).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Execute the code here to start a local Zeno server\n",
    "\n",
    "# from zeno import zeno\n",
    "\n",
    "# from zeno.api import model, distill, metric\n",
    "# from zeno.api import ModelReturn, MetricReturn, DistillReturn, ZenoOptions\n",
    "\n",
    "# @model\n",
    "# def load_model(model_name):\n",
    "    \n",
    "#     def pred(df, ops: ZenoOptions):\n",
    "#         out = df[model_name]\n",
    "#         return ModelReturn(model_output=out)\n",
    "\n",
    "#     return pred\n",
    "\n",
    "# @distill\n",
    "# def label_match(df, ops: ZenoOptions):\n",
    "#     results = (df[ops.label_column] == df[ops.output_column]).to_list()\n",
    "#     return DistillReturn(distill_output=results)\n",
    "\n",
    "# @metric\n",
    "# def accuracy(df, ops: ZenoOptions):\n",
    "#     avg = df[ops.distill_columns[\"label_match\"]].mean()\n",
    "#     return MetricReturn(metric=avg)\n",
    "\n",
    "# zeno({\n",
    "#     \"metadata\": df, # Pandas DataFrame with a row for each instance\n",
    "#     \"view\": \"text-classification\", # The type of view for this data/task\n",
    "#     \"data_column\": \"text\", \n",
    "#     \"label_column\": \"label\",\n",
    "#     \"functions\": [load_model, label_match, accuracy],\n",
    "#     \"models\": [\"roberta\", \"gpt2\"],\n",
    "#     \"port\": 8231\n",
    "# })\n",
    "\n",
    "## If you don't have a Zeno account already, create one on Zeno Hub (https://hub.zenoml.com/signup). \n",
    "## After logging in to Zeno Hub, generate your API key by clicking on your profile at the top right to navigate to your account page.\n",
    "\n",
    "# !pip install zeno_client\n",
    "from zeno_client import ZenoClient, ZenoMetric\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df = df.reset_index()\n",
    "\n",
    "# Initialize a client with the API key.\n",
    "API_KEY = \"zen_voNkqgjxGhPzD4K7cOZgAmKgNgHgYTIEzS4sJKKL83A\"\n",
    "client = ZenoClient(API_KEY)\n",
    "\n",
    "project = client.create_project(\n",
    "    name=\"Tweet Sentiment Analysis\",\n",
    "    view=\"text-classification\",\n",
    "    metrics=[\n",
    "        ZenoMetric(name=\"accuracy\", type=\"mean\", columns=[\"correct\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "project.upload_dataset(df, id_column=\"index\", data_column=\"text\", label_column=\"label\")\n",
    "\n",
    "models = ['roberta', 'gpt2']\n",
    "for model in models:\n",
    "    df_system = df[['index', model]]\n",
    "    \n",
    "    # Measure accuracy for each instance, which is averaged by the ZenoMetric above\n",
    "    df_system[\"correct\"] = (df_system[model] == df[\"label\"]).astype(int)\n",
    "    \n",
    "    project.upload_system(df_system, name=model, id_column=\"index\", output_column=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above, you should be able to access Zeno in http://localhost:8231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After successfully creating the two slices, come up with three *additional* slices you want to check and **create** the slices in the Zeno interface.\n",
    "\n",
    "There are two directions to identify useful slices:\n",
    "- Top-down: Think about what kinds of things the model can struggle with, and come up with some slices.\n",
    "- Bottom-up: Look at model (mis-)predictions, come up with hypotheses, and translate them into data slices.\n",
    "\n",
    "3. [YOUR CHOICE]\n",
    "4. [YOUR CHOICE]\n",
    "5. [YOUR CHOICE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write down descriptions of additional slices you created\n",
    "\n",
    "custom_slice_descriptions = [\n",
    "    \"(not|n't|no|never|none|nowhere|nothing|neither|cannot|can't|won't|isn't|wasn't|shouldn't|couldn't|wouldn't)\",\n",
    "    \"?\"\n",
    "    \"(@\\w+.*@\\w+)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Write down three addition data slices you want to create but do not have the metadata for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, you might have already come up with some slices you wanted to create but found it hard to do with existing metadata. Write down three of such slices in this step.\n",
    "\n",
    "Example: \n",
    "- I want to create a slice on tweets using slangs\n",
    "- I want to create a slice on non-English tweets (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write down three additional data slices here:\n",
    "\n",
    "additional_slice_descriptions = [\n",
    "    \"ä½ å¥½\",\n",
    "    \"ã„\",\n",
    "    \"ã„±\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Generate more test cases with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one slice from the three you wrote down and generate **10 test cases** using LLMs, which can include average case, boundary case, or difficult case.\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - OH: â€œI had a blue penis while I was thisâ€ [playing with Google Earth VR]\n",
    "> - @user @user Thatâ€™s coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "The first part of **Examples** conditions the LLM on the style, length, and content of examples. The second part of **Instructions** instructs what kind of examples you want LLM to generate.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write down the slice you select\n",
    "\n",
    "slice_description = \"ä½ å¥½\"\n",
    "\n",
    "## Write down all generated test cases here\n",
    "\n",
    "generated_test_cases = [\n",
    "    \"ä½ å¥½ï¼ä»Šå¤©çš„å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›å¤§å®¶éƒ½æœ‰ç¾å¥½çš„ä¸€å¤©ï¼â˜€ï¸ğŸ˜Š\"\n",
    "    \"@user ä½ å¥½ï¼Œè¯·é—®è¿™ä¸ªäº§å“è¿˜æœ‰è´§å—ï¼Ÿæƒ³ä¹°ä¸€ä¸ªï¼ğŸ™\"\n",
    "    \"çœ‹åˆ°è¿™æ¡æ¨æ–‡çš„ä½ ï¼Œè®°å¾—è¦å¾®ç¬‘å“¦ï¼ä½ å¥½ï¼Œä¸–ç•Œï¼ğŸŒğŸ’–\"\n",
    "    \"ä½ å¥½ï¼Œæˆ‘åˆšåˆšå¼€å§‹å­¦ä¸­æ–‡ï¼Œæœ‰ä»€ä¹ˆå¥½ç”¨çš„å­¦ä¹ èµ„æºæ¨èå—ï¼ŸğŸ“š\"\n",
    "    \"ä½ å¥½2025ï¼å¸Œæœ›æ–°çš„ä¸€å¹´é‡Œä¸€åˆ‡é¡ºåˆ©ï¼Œæ¢¦æƒ³æˆçœŸï¼ğŸ‰ğŸ¥³\"\n",
    "    \"ä½ å¥½ï¼Œé™Œç”Ÿäººï¼å¸Œæœ›ä½ çš„ä»Šå¤©æ¯”æ˜¨å¤©æ›´ç¾å¥½ï¼ğŸŒŸâœ¨\"\n",
    "    \"ä½ å¥½ï¼ŒåŒ—äº¬ï¼ç»ˆäºæ¥åˆ°äº†è¿™åº§æ¢¦å¯ä»¥æ±‚çš„åŸå¸‚ï¼ğŸ¯ğŸ™ï¸\"\n",
    "    \"ä½ å¥½ï¼Œæˆ‘æ˜¯æ–°æ¥çš„ï¼Œè¯·å¤§å®¶å¤šå¤šå…³ç…§ï¼ğŸ™‹â€â™‚ï¸\"\n",
    "    \"ä½ å¥½ï¼è¯·é—®è¿™ä¸ªæ´»åŠ¨è¿˜æœ‰åé¢å—ï¼Ÿæƒ³æŠ¥åå‚åŠ ï¼ğŸ˜Š\"\n",
    "    \"@user ä½ å¥½ï¼Œæˆ‘å¯¹ä½ çš„ä½œå“å¾ˆæ„Ÿå…´è¶£ï¼Œèƒ½èŠèŠåˆä½œçš„å¯èƒ½æ€§å—ï¼ŸğŸ¨ğŸ’¡\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aie_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
